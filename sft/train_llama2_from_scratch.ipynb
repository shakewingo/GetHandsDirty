{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bdb2b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "import os\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import PretrainedConfig\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer, DefaultDataCollator, DataCollatorForTokenClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711280d3",
   "metadata": {},
   "source": [
    "## Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5f032cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNorm\n",
    "# RoPOE\n",
    "# GPT architecture: attention, mlp, grouped multi-query\n",
    "# HF Transformer framework to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "749d72cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        result = self.weight * (hidden_states / (torch.rsqrt(torch.mean(hidden_states * hidden_states, dim=-1, keepdim=True) + self.variance_epsilon)))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d8af366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoPOE\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotate_pos_emb(q, k, cos, sin, unsqueeze_dim=2):\n",
    "    \n",
    "    cos = cos.unsqueeze(unsqueeze_dim) # (1, seq_len, 1, dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim) # (1, seq_len, 1, dim)\n",
    "   \n",
    "    q_embed = (q*cos) + (rotate_half(q)*sin)  # (batch_size, seq_len, head_num, dim) * (1, seq_len, 1, dim) = (batch_size, seq_len, head_num, dim) 广播\n",
    "    k_embed = (k*cos) + (rotate_half(k)*sin)  # (batch_size, seq_len, head_num, dim) * (1, seq_len, 1, dim) = = (batch_size, seq_len, head_num, dim) 广播\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    # Here is slight different than the slides, originally is [x1, x2, x3, x4,. ...] * [cos(m * theta_1), cos(m * theta_1), cos(m * theta_2), cos(m * theta_2), ..cos(m * theta_d/2)] \n",
    "    # + [-x2, x1, -x4, x3, ...] * [sin(m * theta_1), sin(m * theta_1), sin(m * theta_2), sin(m * theta_2), ..sin(m * theta_d/2)], which is equivalently to be \n",
    "    # [x1, x3, ..., x2, x4,. ...] * [cos(m * theta_1), cos(m * theta_2), ..., cos(m * theta_1), cos(m * theta_2), ..cos(m * theta_d/2)] \n",
    "    # + [-x2, -x4,..., x1, x3, ...] * [sin(m * theta_1), sin(m * theta_2), ..., sin(m * theta_1), sin(m * theta_2), ..sin(m * theta_d/2)]\n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super(RotaryEmbedding, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))  # (dim/2)\n",
    "        t = torch.arange(max_seq_len).float().unsqueeze(1)  # (max_seq_len, 1)\n",
    "        freqs = t @ inv_freq.unsqueeze(0)  #(max_seq_len, 1)*(1, dim/2) = (max_seq_len, dim/2), e.g. m * theta_i part in the slides\n",
    "        freqs = torch.cat((freqs, freqs), dim=-1)  # (max_seq_len, dim)\n",
    "        \n",
    "        self.register_buffer(\"cos_cached\", freqs.cos())\n",
    "        self.register_buffer(\"sin_cached\", freqs.sin())\n",
    "        \n",
    "    def forward(self, q, k):\n",
    "        cos = self.cos_cached[:q.shape[1], :].unsqueeze(0)  # (1, seq_len, dim)\n",
    "        sin = self.sin_cached[:q.shape[1], :].unsqueeze(0)  # (1, seq_len, dim)\n",
    "        return apply_rotate_pos_emb(q, k, cos, sin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fdc48e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config\n",
    "class Config(PretrainedConfig):\n",
    "    model_type = \"custom_gpt\" # for later on: AutoConfig.register(\"custom_gpt\", Config)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=6400,\n",
    "        hidden_size=512,\n",
    "        n_layers = 8,\n",
    "        num_attention_heads=16,\n",
    "        num_key_value_heads = 8,\n",
    "        flash_attn = True,\n",
    "        attention_bias = False,\n",
    "        max_seq_len = 512,\n",
    "        intermediate_size = 2048,\n",
    "        mlp_bias = False,\n",
    "        dropout = 0.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.flash_attn = flash_attn\n",
    "        self.attention_bias = attention_bias\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.mlp_bias = mlp_bias\n",
    "        self.dropout = dropout\n",
    "config = Config()\n",
    "config.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "35d8dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states, num_key_value_groups):\n",
    "    B, S, NUM_KV_H, H = hidden_states.shape # at this moment, the k/v has been linearly projected in consideration of num_key_value_heads\n",
    "    if num_key_value_groups == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, :, None, :].expand(B, S, NUM_KV_H, num_key_value_groups, H)\n",
    "    return hidden_states.reshape(B, S, NUM_KV_H * num_key_value_groups, H)\n",
    "\n",
    "# GPT architecture\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.heads_dim = self.hidden_size // self.num_attention_heads # simpily using default ones instead of customized heads_dim\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_attention_heads // self.num_key_value_heads\n",
    "        self.dropout_prob = config.dropout\n",
    "        self.flash_attn = self.config.flash_attn\n",
    "        self.k_cache, self.v_cache = None, None\n",
    "        self.is_causal = True\n",
    "        self.dropout = nn.Dropout(self.dropout_prob) # simpily using the same value instead of distinguishing attention_dropout and residual_dropout \n",
    "        self.rotary_emb = RotaryEmbedding(self.heads_dim)\n",
    "        \n",
    "        # multi-group transform\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.heads_dim, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.heads_dim, bias=config.attention_bias)\n",
    "\n",
    "    def forward(self, hidden_states, use_kv_cache=False):\n",
    "        B, S, H = hidden_states.shape # H: heads_dim\n",
    "        if use_kv_cache and self.eval(): # model.eval() is used to freeze model in inference phase only\n",
    "            if self.k_cache is None:\n",
    "                q, k, v = hidden_states, self.k_proj(hidden_states), self.v_proj(hidden_states)\n",
    "            else:\n",
    "                last_token = hidden_states[:, -1, :]\n",
    "                q = torch.cat((torch.zeros_like(hidden_states[:, :-1, :]), last_token), dim=1)\n",
    "                k = torch.cat((self.k_cache, self.k_proj(last_token)), dim=1)\n",
    "                v = torch.cat((self.v_cache, self.v_proj(last_token)), dim=1)\n",
    "                \n",
    "                # update kv cache\n",
    "                self.k_cache, self.v_cache = k, v\n",
    "                self.register_buffer(\"k_cache\", self.k_cache)\n",
    "                self.register_buffer(\"v_cache\", self.v_cache)\n",
    "        else:\n",
    "            q, k, v = hidden_states, self.k_proj(hidden_states), self.v_proj(hidden_states)\n",
    "        q = q.view(B, S, self.num_attention_heads, self.heads_dim)\n",
    "        k = k.view(B, S, self.num_key_value_heads, self.heads_dim)\n",
    "        v = v.view(B, S, self.num_key_value_heads, self.heads_dim)\n",
    "        # use RoPOE\n",
    "        q, k = self.rotary_emb(q, k)\n",
    "        # use repetitive k and v for multi-grouped q\n",
    "        k = repeat_kv(k, self.num_key_value_groups)\n",
    "        v = repeat_kv(v, self.num_key_value_groups)\n",
    "\n",
    "        q = q.transpose(1, 2) # (B, NUM_H, S, H)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        if self.flash_attn:\n",
    "            # TODO: 照抄的\n",
    "            # q*k转置，（b, self.num_heads, s, self.head_dim）* (b, self.num_heads, self.head_dim，s) = （b, self.num_heads, s, s）\n",
    "            # q*k/sqrt(self.head_dim)*v  （b, self.num_heads, s, s）* (b, self.num_heads, s, self.head_dim) = b, self.num_heads, s, self.head_dim\n",
    "            output = F.scaled_dot_product_attention(q, k, v, attn_mask=None, \n",
    "                                                    dropout_p=self.dropout_prob if self.training else 0.0, \n",
    "                                                    is_causal=self.is_causal) \n",
    "        else:\n",
    "            mask = torch.full((1, 1, self.config.max_seq_len, self.config.max_seq_len), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim) + mask[:, :, S, S]  # (B, NUM_H, S, H) * (B, NUM_H, H, S) -> (B, NUM_H, S, S)\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(q)  # TODO: not sure why dim=-1. Here considers converting to FP32 then converting back to dtype(q)\n",
    "            scores = self.dropout(scores)\n",
    "            output = torch.matmul(scores, v) # (B, NUM_H, S, H) \n",
    "        output = output.transpose(1, 2).contiguous().view(B, S, self.hidden_size)\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "26b0d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    # Note: why not using nn.Sequential() to implement SwiGLU? - cuz it's not linear pipeline but including parallel structure and a multiplicative operation\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.dropout = config.dropout\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        down_proj = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))\n",
    "        return down_proj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2fc7a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention = Attention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.input_layernorm = RMSNorm(self.hidden_size)\n",
    "        self.post_attention_layernorm = RMSNorm(self.hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, use_kv_cache=False):\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states = self.attention(hidden_states, use_kv_cache)\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.ffn(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "13a9cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(PreTrainedModel):\n",
    "    config_class = Config  # for later on: AutoModelForCausalLM.register(Config, LLM)\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.vocat_size = self.config.vocab_size\n",
    "        self.n_layers = self.config.n_layers\n",
    "        self.dropout = nn.Dropout(self.config.dropout) \n",
    "        self.token_embeddings = nn.Embedding(self.config.vocab_size, self.config.hidden_size)\n",
    "        self.layers = torch.nn.ModuleList() \n",
    "        for _ in range(self.n_layers):\n",
    "            self.layers.append(DecoderLayer(config)) \n",
    "        self.layernorm = RMSNorm(self.config.hidden_size)\n",
    "        self.output = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False) # each token generated's shape is (hidden_size, vocab_size)\n",
    "        self.apply(self._init_weights) \n",
    "        self.loss = None \n",
    "\n",
    "        # TODO: have no idea why it looks like this, looks so hacky - explained by GPT: \n",
    "        # the loop over self.named_parameters() looks for tensor names ending with w3.weight (the MLP’s down-projection in a SwiGLU block) \n",
    "        # or wo.weight (the attention output projection) and rescales them with a smaller std, 0.02 / sqrt(2 * n_layers), to match the RMSNorm-residual scaling used in LLaMA-style models.\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layers)) \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)  \n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)  \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02) \n",
    "\n",
    "\n",
    "    def forward(self, input_ids, labels, use_kv_cache=False):\n",
    "        hidden_states = self.token_embeddings(input_ids)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, use_kv_cache=use_kv_cache)  \n",
    "        hidden_states = self.layernorm(hidden_states) \n",
    "\n",
    "        if labels is not None:\n",
    "            logits = self.output(hidden_states)  \n",
    "            self.loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=0) \n",
    "        else:\n",
    "            # for inference\n",
    "            logits = self.output(hidden_states[:, [-1], :])    \n",
    "            self.loss = None  \n",
    "        \n",
    "        return CausalLMOutputWithPast(self.loss, logits) # meaning can call LLM().loss, LLM.logits directly\n",
    "    \n",
    "    @torch.inference_mode\n",
    "    def generate(self, inputs, eos, max_new_tokens, temperature=0.7, top_k=None, stream=True, repetition_penalty=1.,\n",
    "                 use_kv_cache=True):\n",
    "        \n",
    "        input_ids = inputs['input_ids']\n",
    "        labels = inputs['labels']\n",
    "        s = input_ids.shape[1]\n",
    "        while input_ids.shape[1] < max_new_tokens - 1:  \n",
    "            inference_res = self.forward(input_ids, labels, use_kv_cache=use_kv_cache)  \n",
    "            logits = inference_res.logits \n",
    "            logits = logits[:, -1, :] \n",
    "\n",
    "            # apply penaly for repetitive tokens\n",
    "            for token in set(input_ids.tolist()[0]):  \n",
    "                logits[:, token] /= repetition_penalty\n",
    "\n",
    "            if temperature == 0.0: \n",
    "                _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "            else:\n",
    "                logits = logits / temperature  \n",
    "                if top_k is not None:  \n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf') \n",
    "\n",
    "                probs = F.softmax(logits, dim=-1)  \n",
    "                idx_next = torch.multinomial(probs, num_samples=1, generator=None)  \n",
    "\n",
    "            if idx_next == eos:  \n",
    "                break\n",
    "\n",
    "            input_ids = torch.cat((input_ids, idx_next), dim=1)  \n",
    "            if stream:  \n",
    "                yield input_ids[:, s:]  \n",
    "\n",
    "        if not stream:  \n",
    "            yield input_ids[:, s:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "317a9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./tokenizer\")\n",
    "tokenizer.bos_token = '<|im_start|>' # based on original data\n",
    "tokenizer.eos_token = '<|im_end|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fca912a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_end|>'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9e99b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMDataset(IterableDataset):\n",
    "    def __init__(self, data_path, tokenizer, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.data_process()\n",
    "    \n",
    "    def data_process(self):\n",
    "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = json.loads(line)\n",
    "                text = line['text']\n",
    "                input_ids = self.tokenizer.encode(text)\n",
    "                text_len = len(input_ids)\n",
    "                if text_len > self.max_seq_len:\n",
    "                    input_ids = input_ids[:self.max_seq_len]\n",
    "                else:\n",
    "                    input_ids = input_ids + [0] * (self.max_seq_len - text_len)\n",
    "                input_ids = np.array(input_ids)\n",
    "                X = np.array(input_ids[:-1]).astype(np.int64)\n",
    "                Y = np.array(input_ids[1:]).astype(np.int64)\n",
    "                yield {\n",
    "                    'input_ids': torch.from_numpy(X),\n",
    "                    'labels': torch.from_numpy(Y),\n",
    "                }\n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "22ccd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LLMDataset(\"./dataset/pretrain_hq.jsonl\", tokenizer, max_seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "14010dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "model = LLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d8013d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33825280"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926786dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(output_dir='./result', \n",
    "                        num_train_epochs=2, \n",
    "                        do_train=True, \n",
    "                        per_device_train_batch_size=2,\n",
    "                        gradient_accumulation_steps=1,\n",
    "                        group_by_length=False,\n",
    "                        max_steps=1000,\n",
    "                        logging_steps=100,\n",
    "                        report_to = 'none')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2af9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingyao/miniconda3/envs/transformer-practice/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 02:43, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.454600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>5.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.546100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingyao/miniconda3/envs/transformer-practice/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 02:42, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.896500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.442600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.880200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.768300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.301900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=4.83419515991211, metrics={'train_runtime': 162.6451, 'train_samples_per_second': 12.297, 'train_steps_per_second': 6.148, 'total_flos': 187323279360000.0, 'train_loss': 4.83419515991211, 'epoch': 1.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DefaultDataCollator()       \n",
    "trainer = Trainer(model=model, args=args, train_dataset=dataset, processing_class=tokenizer, data_collator=data_collator)\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4226c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./model')\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e46332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1,  731,   14,   20, 6239, 1919,   34]]),\n",
       " 'labels': None}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval result\n",
    "AutoConfig.register(\"custom_gpt\", Config)\n",
    "AutoModelForCausalLM.register(Config, LLM)\n",
    "reload_model = AutoModelForCausalLM.from_pretrained('./model')  # './model/sft'\n",
    "input_ids = [tokenizer.bos_token_id] + tokenizer.encode(\"1+1等于几?\")\n",
    "input_data = {'input_ids': torch.tensor(input_ids).unsqueeze(0), \"labels\":None} # unsqueeze(0) to insert a dim at index 0 for batch\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0f073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " def所 tour sourceov钱风格反馈自然hel saf run he整数 tool are workers/父母宇宙主�oldDales控制 jo充分拥有self�erciscomes多 favor描述攻nowR毒 int diagn perform咨询 comes树 cultures integrider暖day阳光 populargedial regard meaning John最 Trans effects maj范 clo19要求浪ses另 sharingivid理论oster who rep现在在一个 pers ele注政策位 population simple词真建立并pt products�发展\n"
     ]
    }
   ],
   "source": [
    "for token in reload_model.generate(inputs=input_data, eos=tokenizer.eos_token_id, max_new_tokens=100, stream=False):\n",
    "    print(tokenizer.decode(token[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6908ba",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d56e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1105, -0.4911,  0.1613,  ...,  0.8285, -0.3369,  0.2555],\n",
      "         [-0.3322,  2.6126,  0.8440,  ..., -1.1634, -0.8469,  2.3062],\n",
      "         [-1.2211, -0.3163,  0.6839,  ..., -0.7464, -1.4905,  1.1278],\n",
      "         ...,\n",
      "         [ 2.8208, -0.5343,  1.3579,  ..., -1.1893, -0.1555, -0.4554],\n",
      "         [ 0.9785,  1.2703, -1.8128,  ..., -0.1759, -0.0936, -0.4683],\n",
      "         [ 0.4855,  0.7175,  0.9907,  ...,  0.7397, -0.5728,  0.2728]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# rmsnorm = RMSNorm(hidden_size=768)\n",
    "# torch.manual_seed(1234)\n",
    "# result = rmsnorm.forward(torch.randn(1, 1024, 768))\n",
    "# # print(rmsnorm.weight)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eca48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# dim = 768\n",
    "# max_seq_len=2048\n",
    "# inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))  # 形状(dim/2)\n",
    "# t = torch.arange(max_seq_len).float().unsqueeze(1)  # 形状(max_seq_len, 1)\n",
    "\n",
    "# freqs = t @ inv_freq.unsqueeze(0)  #(max_seq_len, 1)*(1, dim/2) = (max_seq_len, dim/2)\n",
    "# print(t)\n",
    "# print(freqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71174ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = torch.randn((2, 3, 4, 5))\n",
    "# q = torch.randn((2, 3, 4, 5))\n",
    "# v = torch.randn((2, 3, 4, 5))\n",
    "# mask = torch.randn((2, 3, 6, 6))\n",
    "# mask = torch.triu(mask, diagonal=1)\n",
    "# scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c529e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' <|im_start|>鉴别一组中文文章的风格和特点，例如官方、口语、文言等。需要提供样例文章才能准确鉴别不同的风格和特点。<|im_end|> <|im_start|>好的，现在帮我查一下今天的天气怎么样?今天的天气依据地区而异。请问你需要我帮你查询哪个地区的天气呢？<|im_end|> <|im_start|>打开闹钟功能，定一个明天早上七点的闹钟。好的，我已经帮您打开闹钟功能，闹钟将在明天早上七点准时响起。<|im_end|> <|im_start|>为以下场景写一句话描述：一个孤独的老人坐在公园长椅上看着远处。一位孤独的老人坐在公园长椅上凝视远方。<|im_end|> <|im_start|>非常感谢你的回答。请告诉我，这些数据是关于什么主题的？这些数据是关于不同年龄段的男女人口比例分布的。<|im_end|> <|im_start|>帮我想一个有趣的标题。这个挺有趣的：\"如何成为一名成功的魔术师\" 调皮的标题往往会吸引读者的注意力。<|im_end|> <|im_start|>回答一个问题，地球的半径是多少？地球的平均半径约为6371公里，这是地球自赤道到两极的距离的平均值。<|im_end|> <|im_start|>识别文本中的语气，并将其分类为喜悦、悲伤、惊异等。\\n文本：“今天是我的生日！”这个文本的语气是喜悦。<|im_end|><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # encoded input looks like: \n",
    "# data_iter = iter(dataset)\n",
    "# print(type(data_iter))\n",
    "# sample = next(data_iter)\n",
    "# input_ids = sample['input_ids']\n",
    "# tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9cb6164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 6400])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids = torch.randint(0, 10, (2, 512)) # min_int, max_int, (S, H)\n",
    "# labels = torch.randint(0, 10, (2, 512))\n",
    "# model(input_ids, labels).logits.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
