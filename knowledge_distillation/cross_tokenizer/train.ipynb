{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40916ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingyao/miniconda3/envs/transformer-practice/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import IterableDataset, Dataset\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DefaultDataCollator, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf121304",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5620e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/yingyao/Desktop/Code/GetHandsDirty.nosync/knowledge_distillation/cross_tokenizer/example.json'\n",
    "STD_MODEL_PATH = '/Users/yingyao/Desktop/Code/GetHandsDirty.nosync/gz-data/Qwen2.5-0.5B-Instruct'\n",
    "TCH_MODEL_PATH = '/Users/yingyao/Desktop/Code/GetHandsDirty.nosync/gz-data/Qwen2.5-1.5B-Instruct'# GLM-4-9B-0414\n",
    "OUTPUT_DIR = '/Users/yingyao/Desktop/Code/GetHandsDirty.nosync/knowledge_distillation/result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48636e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prompt = item['prompt']\n",
    "        answer = item['answer']\n",
    "   \n",
    "        prompt_ids = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        answer_ids = self.tokenizer.encode(answer, add_special_tokens=False)\n",
    "        \n",
    "        input_ids = prompt_ids + answer_ids\n",
    "        labels = answer_ids\n",
    "    \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_model = AutoModelForCausalLM.from_pretrained(STD_MODEL_PATH, local_files_only=True)\n",
    "tch_model = AutoModelForCausalLM.from_pretrained(TCH_MODEL_PATH, local_files_only=True)\n",
    "std_tokenizer = AutoTokenizer.from_pretrained(STD_MODEL_PATH, use_fast=True, fix_mistral_regex=True)\n",
    "tch_tokenizer = AutoTokenizer.from_pretrained(TCH_MODEL_PATH, use_fast=True, fix_mistral_regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d2805c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [105043, 100165, 11319, 35946, 101909, 15469, 110498, 1773],\n",
       " 'labels': [35946, 101909, 15469, 110498, 1773]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = SFTDataset(DATA_PATH, std_tokenizer)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff05a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你是谁？我是一个AI助手。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_tokenizer.decode(dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c4def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=std_tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e7c9569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[105043, 100165,  11319,  35946, 101909,  15469, 110498,   1773, 151643,\n",
       "         151643],\n",
       "        [ 56568,  99882,  99245, 101419,  11319,  35946,  99882,  30709,  99473,\n",
       "           1773]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 35946, 101909,  15469, 110498,   1773],\n",
       "        [ 35946,  99882,  30709,  99473,   1773]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(features = [dataset[0], dataset[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "299c26f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151665, 151665)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_vocab = std_tokenizer.get_vocab()\n",
    "tch_vocab = tch_tokenizer.get_vocab()\n",
    "len(std_vocab), len(tch_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69b12efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vocab_mapping(std_tokenizer, tch_tokenizer):\n",
    "    \"\"\"\n",
    "    Returns: vacab_mapping: dict mapping teacher token IDs to student token IDs\n",
    "             teacher_matched_ids: set of matched teacher token IDs\n",
    "             student_matched_ids: set of matched student token IDs\n",
    "    \"\"\"\n",
    "\n",
    "    student_vocab = std_tokenizer.get_vocab()\n",
    "    teacher_vocab = tch_tokenizer.get_vocab()\n",
    "    \n",
    "    student_token_to_id = dict(student_vocab.items())\n",
    "    vocab_mapping = {}\n",
    "    \n",
    "    teacher_matched_ids = set()\n",
    "    student_matched_ids = set()\n",
    "\n",
    "    for token_str, teacher_token_id in teacher_vocab.items():\n",
    "        if token_str in student_token_to_id:\n",
    "            student_token_id = student_token_to_id[token_str]\n",
    "            vocab_mapping[teacher_token_id] = student_token_id\n",
    "            teacher_matched_ids.add(teacher_token_id)\n",
    "            student_matched_ids.add(student_token_id)\n",
    "\n",
    "    return vocab_mapping, teacher_matched_ids, student_matched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92cd2bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151665, 151665)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_mapping, teacher_matched_ids, student_matched_ids = init_vocab_mapping(std_tokenizer, tch_tokenizer)\n",
    "len(teacher_matched_ids), len(student_matched_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f47ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ULDLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, std_tokenizer, tch_tokenizer, temperature=0.2):\n",
    "        super().__init__()\n",
    "        self.std_tokenizer = std_tokenizer\n",
    "        self.tch_tokenizer = tch_tokenizer\n",
    "        self.temperature = temperature\n",
    "        vocab_mapping, teacher_matched_ids, student_matched_ids = (\n",
    "            self.init_vocab_mapping()\n",
    "        )\n",
    "        self.vocab_mapping = vocab_mapping\n",
    "        self.teacher_matched_ids = teacher_matched_ids\n",
    "        self.student_matched_ids = student_matched_ids\n",
    "\n",
    "    def init_vocab_mapping(self):\n",
    "        \"\"\"\n",
    "        Returns: vacab_mapping: dict mapping teacher token IDs to student token IDs\n",
    "                teacher_matched_ids: set of matched teacher token IDs\n",
    "                student_matched_ids: set of matched student token IDs\n",
    "        \"\"\"\n",
    "\n",
    "        student_vocab = self.std_tokenizer.get_vocab()\n",
    "        teacher_vocab = self.tch_tokenizer.get_vocab()\n",
    "\n",
    "        student_token_to_id = dict(student_vocab.items())\n",
    "        vocab_mapping = {}\n",
    "\n",
    "        teacher_matched_ids = set()\n",
    "        student_matched_ids = set()\n",
    "\n",
    "        for token_str, teacher_token_id in teacher_vocab.items():\n",
    "            if token_str in student_token_to_id:\n",
    "                student_token_id = student_token_to_id[token_str]\n",
    "                vocab_mapping[teacher_token_id] = student_token_id\n",
    "                teacher_matched_ids.add(teacher_token_id)\n",
    "                student_matched_ids.add(student_token_id)\n",
    "\n",
    "        return vocab_mapping, teacher_matched_ids, student_matched_ids\n",
    "\n",
    "    def get_alignment_groups_from_ids(self, std_token_ids, tch_token_ids):\n",
    "\n",
    "        def to_canonical_pieces(tok, ids):\n",
    "            pieces = []\n",
    "            prev = \"\"\n",
    "            for k in range(len(ids)):\n",
    "                cur = tok.decode(\n",
    "                    ids[: k + 1],\n",
    "                    skip_special_tokens=False,\n",
    "                    clean_up_tokenization_spaces=False,\n",
    "                )\n",
    "                pieces.append(cur[len(prev) :])\n",
    "                prev = cur\n",
    "            return pieces\n",
    "\n",
    "        s_pieces = to_canonical_pieces(self.student_tokenizer, std_token_ids)\n",
    "        t_pieces = to_canonical_pieces(self.teacher_tokenizer, tch_token_ids)\n",
    "\n",
    "        i = j = 0\n",
    "        s_buf = t_buf = \"\"\n",
    "        s_group = []\n",
    "        t_group = []\n",
    "        s_groups = []\n",
    "        t_groups = []\n",
    "\n",
    "        def flush():\n",
    "            if s_group and t_group:\n",
    "                s_groups.append(s_group.copy())\n",
    "                t_groups.append(t_group.copy())\n",
    "\n",
    "        while i < len(s_pieces) or j < len(t_pieces):\n",
    "            if s_buf == t_buf and s_buf != \"\":\n",
    "                flush()\n",
    "                s_buf = t_buf = \"\"\n",
    "                s_group = []\n",
    "                t_group = []\n",
    "                continue\n",
    "\n",
    "            if s_buf == \"\" and i < len(s_pieces):\n",
    "                s_buf += s_pieces[i]\n",
    "                s_group.append(i)\n",
    "                i += 1\n",
    "                continue\n",
    "            if t_buf == \"\" and j < len(t_pieces):\n",
    "                t_buf += t_pieces[j]\n",
    "                t_group.append(j)\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "            if len(s_buf) <= len(t_buf):\n",
    "                if i < len(s_pieces):\n",
    "                    s_buf += s_pieces[i]\n",
    "                    s_group.append(i)\n",
    "                    i += 1\n",
    "                elif j < len(t_pieces):\n",
    "                    t_buf += t_pieces[j]\n",
    "                    t_group.append(j)\n",
    "                    j += 1\n",
    "            else:\n",
    "                if j < len(t_pieces):\n",
    "                    t_buf += t_pieces[j]\n",
    "                    t_group.append(j)\n",
    "                    j += 1\n",
    "                elif i < len(s_pieces):\n",
    "                    s_buf += s_pieces[i]\n",
    "                    s_group.append(i)\n",
    "                    i += 1\n",
    "\n",
    "        if s_buf == t_buf and s_group and t_group:\n",
    "            flush()\n",
    "        elif s_group or t_group:\n",
    "\n",
    "            if s_group or t_group:\n",
    "                if not s_group:\n",
    "                    s_group = []\n",
    "                if not t_group:\n",
    "                    t_group = []\n",
    "                if s_group or t_group:\n",
    "                    s_groups.append(s_group.copy() if s_group else [])\n",
    "                    t_groups.append(t_group.copy() if t_group else [])\n",
    "\n",
    "        return s_groups, t_groups\n",
    "\n",
    "    def merge_prob_with_alignment_groups(self, probs, alignment_groups):\n",
    "\n",
    "        if not alignment_groups:\n",
    "            return probs\n",
    "\n",
    "        vocab_size = probs.size(-1)\n",
    "        target_len = len(alignment_groups)\n",
    "        aligned_probs = torch.zeros(target_len, vocab_size, device=probs.device)\n",
    "\n",
    "        for group_idx, group in enumerate(alignment_groups):\n",
    "            if len(group) > 1:\n",
    "                eps = 1e-8\n",
    "                logp = torch.log(probs[group[0]].clamp_min(eps))\n",
    "                for idx in group[1:]:\n",
    "                    if idx < probs.size(0):\n",
    "                        logp = logp + torch.log(probs[idx].clamp_min(eps))\n",
    "                aligned_probs[group_idx] = torch.softmax(logp, dim=-1)\n",
    "            elif len(group) == 1:\n",
    "                aligned_probs[group_idx] = probs[group[0]]\n",
    "            else:\n",
    "                aligned_probs[group_idx] = torch.zeros_like(probs[0])\n",
    "\n",
    "        return aligned_probs\n",
    "    \n",
    "    def get_answer_start_and_len(self, answers, tokenizer) -> Tuple[List[int], List[int]]:\n",
    "        answers_index = []\n",
    "        answers_size = []\n",
    "\n",
    "        for answer in answers:\n",
    "            answer_mask = answer.ne(tokenizer.pad_token_id)\n",
    "            if not answer_mask.any():\n",
    "                answers_index.append(0)\n",
    "                answers_size.append(0)\n",
    "                continue\n",
    "\n",
    "            indices = answer_mask.nonzero(as_tuple=True)[0]\n",
    "            answers_index.append(int(indices[0].item()))\n",
    "            answers_size.append(int(answer_mask.sum().item()))\n",
    "        return answers_index, answers_size\n",
    "    \n",
    "    def compute_uld_loss(self, std_logits, tch_logits, std_labels, tch_labels, std_input_ids, tch_input_ids):\n",
    "        # align text length\n",
    "        std_ans_index, std_ans_size = self.get_answer_start_and_len(std_logits, self.std_tokenizer)\n",
    "        tch_ans_index, tch_ans_size = self.get_answer_start_and_len(tch_logits, self.tch_tokenizer)\n",
    "        B = std_logits.shape[0]\n",
    "        for b in range(B):\n",
    "            # keep only ans part\n",
    "            std_ans_logits = std_logits[b, std_ans_index[b] : std_ans_index[b] + std_ans_size[b], :] \n",
    "            tch_ans_logits = tch_logits[b, tch_ans_index[b] : tch_ans_index[b] + tch_ans_size[b], :] \n",
    "\n",
    "            student_probs = F.softmax(std_ans_logits / self.temperature, dim=-1)\n",
    "            teacher_probs = F.softmax(tch_ans_logits / self.temperature, dim=-1)\n",
    "\n",
    "            std_token_ids = std_input_ids[b, std_ans_index[b] : std_ans_index[b] + std_ans_size[b]].tolist()  \n",
    "            tch_token_ids = tch_input_ids[b, tch_ans_index[b] : tch_ans_index[b] + tch_ans_size[b]].tolist()\n",
    "            std_alignment_groups, tch_alignment_groups = self.get_alignment_groups_from_ids(std_token_ids[:-1], tch_token_ids[:-1])\n",
    "            std_aligned = self.merge_prob_with_alignment_groups(student_probs[:-1], std_alignment_groups)\n",
    "            tch_aligned = self.merge_prob_with_alignment_groups(teacher_probs[:-1], tch_alignment_groups)\n",
    "            std_aligned = torch.cat([std_aligned, student_probs[-1:, :]], dim=0)\n",
    "            tch_aligned = torch.cat([tch_aligned, teacher_probs[-1:, :]], dim=0)\n",
    "\n",
    "        # align vocab size - use KL loss to train for matched tokens; use sort + pad and L1 loss to train for unmatched tokens\n",
    "        return None\n",
    "\n",
    "    def forward(self, std_logits, tch_logits, std_labels, tch_labels, std_input_ids, tch_input_ids):\n",
    "        loss = self.compute_uld_loss(std_logits, tch_logits, std_labels, tch_labels, std_input_ids, tch_input_ids)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfe0636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDTrainer(Trainer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,\n",
    "        tch_model=None,\n",
    "        tch_tokenizer=None,\n",
    "        args=None,\n",
    "        data_collator=None,\n",
    "        train_dataset=None,\n",
    "        tokenizer=None,\n",
    "        max_length=512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.tch_model = tch_model.eval()\n",
    "        self.tch_tokenizer = tch_tokenizer\n",
    "        self.max_length = max_length\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.uld_loss = ULDLoss(std_tokenizer=tokenizer, tch_tokenizer=tch_tokenizer)\n",
    "\n",
    "    def get_inputs_from_text(self, tokenizer, prompt_texts, ans_texts):\n",
    "        sequences = []\n",
    "        labels_list = []\n",
    "        attention_masks = []\n",
    "        for prompt_text, ans_text in zip(prompt_texts, ans_texts):\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                messages, add_generation_prompt=True, tokenize=False\n",
    "            )\n",
    "            prompt_ids = tokenizer.encode(prompt)\n",
    "            answer_ids = tokenizer.encode(ans_text, add_special_tokens=False) + [\n",
    "                tokenizer.eos_token_id\n",
    "            ]\n",
    "            sequence = prompt_ids + answer_ids\n",
    "            attention_mask = [1] * len(sequence)\n",
    "            labels = [tokenizer.pad_token_id] * len(prompt_ids) + answer_ids\n",
    "            if len(sequence) > self.max_length:\n",
    "                sequence = sequence[: self.max_length]\n",
    "                attention_mask = attention_mask[: self.max_length]\n",
    "                labels = labels[: self.max_length]\n",
    "            else:\n",
    "                sequence += tokenizer.pad_token_id * (self.max_length - len(sequence))\n",
    "                attention_mask += tokenizer.pad_token_id * (self.max_length - len(sequence))\n",
    "                labels += tokenizer.pad_token_id * (self.max_length - len(sequence))\n",
    "            sequences.append(torch.tensor(sequence))\n",
    "            attention_masks.append(torch.tensor(attention_mask))\n",
    "            labels_list.append(torch.tensor(labels))\n",
    "        sequences = torch.stack(sequences).contiguous().to(self.model.device)\n",
    "        attention_masks = torch.stack(attention_masks).contiguous().to(self.model.device)\n",
    "        labels = torch.stack(labels_list).contiguous().to(self.model.device)\n",
    "        return sequences, attention_masks, labels\n",
    "\n",
    "\n",
    "    def compute_loss(\n",
    "        self, model, inputs, return_outputs=False, num_items_in_batch=None\n",
    "    ):\n",
    "        input_ids = inputs['input_ids']\n",
    "        labels = inputs['labels']\n",
    "        prompt_ids = [input_id[:len(input_id) - len(label)] for input_id, label in zip(input_ids, labels)]\n",
    "        \n",
    "        prompt_texts = self.tokenizer.batch_decode(prompt_ids)\n",
    "        answer_texts = self.tokenizer.batch_decode(labels)\n",
    "\n",
    "        std_input_ids, std_labels, std_attention_mask  = self.get_inputs_from_text(self.tokenizer, prompt_texts, answer_texts)\n",
    "        tch_input_ids, tch_labels, tch_attention_mask = self.get_inputs_from_text(self.tch_tokenizer, prompt_texts, answer_texts)\n",
    "\n",
    "        outputs = model(input_ids = std_input_ids, attention_mask=std_attention_mask)\n",
    "        with torch.no_grad():\n",
    "            tch_outputs = self.tch_model(input_ids = tch_input_ids, attention_mask=tch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "        tch_logits = tch_outputs.logits\n",
    "\n",
    "        loss = self.uld_loss(logits, tch_logits, std_labels, tch_labels, std_attention_mask, tch_attention_mask)\n",
    "        print(f\"loss: {loss:.4f}\")\n",
    "        return (loss, logits) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cd85f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(output_dir=OUTPUT_DIR, \n",
    "                        num_train_epochs=1, \n",
    "                        do_train=True, \n",
    "                        per_device_train_batch_size=8,\n",
    "                        gradient_accumulation_steps=1,\n",
    "                        logging_steps=1,\n",
    "                        report_to='tensorboard',\n",
    "                        save_strategy='steps',\n",
    "                        save_total_limit=3,\n",
    "                        save_steps=100,\n",
    "                        bf16=True,\n",
    "                        learning_rate=0.00001,\n",
    "                        lr_scheduler_type='cosine',\n",
    "                        dataloader_num_workers=8,\n",
    "                        dataloader_pin_memory=True,\n",
    "                        max_steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/k0c2nyfd70b5x3jcncjlfctr0000gn/T/ipykernel_37004/2447436402.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `KDTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "/Users/yingyao/miniconda3/envs/transformer-practice/lib/python3.14/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "ERROR:tornado.general:SEND Error: Host unreachable\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    import sys; sys.path.insert(0, r'/Users/yingyao/miniconda3/envs/transformer-practice/lib/python3.14/site-packages/debugpy/_vendored/pydevd'); import pydevd; pydevd.config('http_json', 'debugpy-dap'); pydevd.settrace(host='127.0.0.1', port=56188, suspend=False, trace_only_current_thread=False, patch_multiprocessing=True, access_token='e8a815cf8e9dbb204d5671ef0f0ede30922532697df5d5765862d0bd7a259595', client_access_token=None, __setup_holder__={'access-token': 'e8a815cf8e9dbb204d5671ef0f0ede30922532697df5d5765862d0bd7a259595', 'client': '127.0.0.1', 'debug-mode': 'debugpy-dap', 'json-dap-http': True, 'multiprocess': True, 'port': 56188, 'ppid': 37004, 'preimport': '/Users/yingyao/miniconda3/envs/transformer-practice/lib/python3.14/site-packages;debugpy._vendored.force_pydevd', 'server': False, 'skip-notify-stdin': True}); from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=100, pipe_handle=114)\u001b[0m\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/yingyao/miniconda3/envs/transformer-practice/lib/python3.14/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/Users/yingyao/miniconda3/envs/transformer-practice/lib/python3.14/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mmodule '__main__' has no attribute 'SFTDataset'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "trainer = KDTrainer(model=std_model,\n",
    "                    tch_model=tch_model, \n",
    "                    args=args, \n",
    "                    train_dataset=dataset, \n",
    "                    tokenizer=std_tokenizer, \n",
    "                    tch_tokenizer=tch_tokenizer,\n",
    "                    data_collator=data_collator)\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8412ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
