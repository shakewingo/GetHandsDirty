# %%
"""
Author:
Date: 25/12/24
"""

from dataclasses import dataclass
import random
import gc
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
from transformers import (
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    AutoTokenizer,
)
from typing import List, Callable, Dict, Union, Optional
from tqdm import tqdm
from datasets import load_dataset
from transformers import (
    Trainer,
    TrainingArguments,
    AutoModelForCausalLM,
    AutoTokenizer,
    AutoModelForSequenceClassification,
)
from copy import deepcopy

# from trl.trainer import grpo_trainer
from grpo.grpo_reward_func import (
    correctness_reward,
    digit_reward,
    hard_format_reward,
    mark_reward,
)


# %%
class GSM8KDataset(Dataset):
    def __init__(self, data_path, tokenizer):

        self.tokenizer = tokenizer
        data = load_dataset(data_path)
        self.data = data["train"]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        sample = self.data[index]
        # prompt = self.tokenizer.apply_chat_template(sample['prompt'], tokenize=False, add_generation_prompt=True)
        answer = sample["answer_only"]
        prompt = sample["question"]
        return {"prompt": prompt, "answer": answer}


# %%
@dataclass
class Samples:
    prompt_response_ids: torch.Tensor
    response_ids: torch.Tensor
    prompt: str
    answer: str
    attention_mask: Optional[torch.LongTensor]
    action_mask: Optional[torch.BoolTensor]
    num_actions: Union[torch.Tensor, int]
    response_length: torch.Tensor


@dataclass
class Experience:
    prompt_response_ids: torch.Tensor
    action_log_probs: torch.Tensor
    advantages: Optional[torch.Tensor]
    attention_mask: Optional[torch.LongTensor]
    action_mask: Optional[torch.BoolTensor]
    num_actions: Union[int, torch.Tensor]
    # kl: Optional[torch.Tensor] = None


# %%
class GRPOTrainer:
    def __init__(
        self,
        model,
        args,
        train_dataset,
        tokenizer,
        reward_funcs: List[Callable],  # here doesn't include loaded reward model from_pretrained
        reward_weights: List[float],
        ref_model=None,
        **kwargs,
    ):
        self.model = model
        print(f"model_device: {self.model.device}")
        self.args = args
        self.ref_model = (
            ref_model.eval() if ref_model is not None else deepcopy(model)
        )  # otherwise, ref_model will be impacted by model updates
        self.ref_model.eval()
        self.train_dataset = train_dataset
        self.tokenizer = tokenizer
        self.tokenizer.padding_side = "left"
        self.reward_weigths = reward_weights
        self.reward_funcs = reward_funcs
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)
        self.max_steps = (
            self.args.max_steps if hasattr(self.args, "max_steps") else None
        )
        self.exp_buffer = [None] * self.args.gradient_accumulation_steps # for off-policy usage, storing repsonses generated by old model

    def generate_samples(
        self,
        inputs: List[Dict[str, str]],
    ) -> List[Samples]:
        samples_list = []
        self.model.eval()
        for prompt, answer in zip(inputs["prompt"], inputs["answer"]):
            total_length = self.args.max_generate_length + self.args.max_prompt_length
            input_text = self.tokenizer.apply_chat_template(
                [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt},
                ],
                add_generation_prompt=True,
                tokenize=False,
            )
            inputs = self.tokenizer(
                input_text,
                padding="max_length",
                max_length=self.args.max_prompt_length,
                truncation=True,
                return_tensors="pt",
            )
            input_ids = inputs["input_ids"]
            with torch.no_grad():
                prompt_response_ids = self.model.generate(
                    **inputs.to(self.model.device),
                    max_new_tokens=self.args.max_generate_length,
                    eos_token_id=self.tokenizer.eos_token_id,
                    pad_token_id=self.tokenizer.pad_token_id,
                    num_return_sequences=self.args.num_samples,  # generate a group responses
                    do_sample=True,
                )
            if prompt_response_ids.size(1) >= total_length:
                prompt_response_ids = prompt_response_ids[:, :total_length]
            else:
                prompt_response_ids = torch.cat(
                    [
                        prompt_response_ids,
                        torch.full(
                            (
                                prompt_response_ids.size(0),
                                total_length - prompt_response_ids.size(1),
                            ),
                            fill_value=self.tokenizer.pad_token_id,
                            device=prompt_response_ids.device,
                        ),
                    ],
                    dim=1,
                )
            attention_mask = (prompt_response_ids.ne(self.tokenizer.pad_token_id)).to(
                dtype=torch.long
            )  # masked since beginning of the prompt
            response_ids = prompt_response_ids[:, input_ids.size(1) :]
            action_mask = (
                response_ids.ne(self.tokenizer.pad_token_id)
                & response_ids.ne(self.tokenizer.eos_token_id)
            ).to(
                dtype=torch.long
            )  # masked since beginning of the answer

            # each samples is a "group" per prompt in GRPO
            samples = Samples(
                prompt_response_ids=prompt_response_ids,  # shape: (num_sample_generations, total_length)
                response_ids=response_ids,
                prompt=prompt,  # shape: (1)
                answer=answer,  # shape: (1)
                attention_mask=attention_mask,  # shape: (num_sample_generations, total_length)
                action_mask=action_mask,  # shape: (num_sample_generations, max_generate_length)
                num_actions=action_mask.size(1),  # shape: (1)
                response_length=action_mask.float().sum(
                    dim=-1
                ),  # shape: (num_sample_generations, 1)
            )
            samples_list.append(samples)
        return samples_list

    def generate_experiences(self, samples_list: List[Samples]):
        self.model.eval()
        experiences = []
        for samples in samples_list:
            prompt_response_ids = (
                samples.prompt_response_ids
            )  # shape: (num_sample_generations, total_length)
            prompt = samples.prompt  # shape: (1)
            answer = samples.answer  # shape: (1)
            response_ids = samples.response_ids
            attention_mask = samples.attention_mask
            action_mask = samples.action_mask
            num_actions = samples.num_actions  # shape: (1)
            with torch.no_grad():
                # get output logits
                outputs = self.model(prompt_response_ids, attention_mask=attention_mask)
                log_probs = F.softmax(outputs.logits[:, :-1, :], dim=-1)  # (B, S, V)
                log_probs_labels = log_probs.gather(
                    dim=-1, index=prompt_response_ids[:, 1:].unsqueeze(-1)
                )  # (B, S, 1)
                action_log_probs = log_probs_labels.squeeze(-1)[
                    :, -num_actions:
                ]  # truncate for only output part

                responses = self.tokenizer.batch_decode(
                    response_ids, skip_special_tokens=True
                )

                # calculate reward
                # here ignores weights across different reward functions
                prompts = [prompt] * response_ids.size(0)
                answers = [answer] * response_ids.size(0)
                final_reward = torch.zeros(
                    response_ids.size(0), device=self.model.device
                )
                for reward_func, reward_weight in zip(
                    self.reward_funcs, self.reward_weigths
                ):
                    reward = reward_func(prompts, responses, answers)
                    # Convert list to tensor and accumulate
                    final_reward += reward_weight * torch.tensor(
                        reward, device=self.model.device, dtype=torch.float
                    )
                print(f"grouped rewards: {final_reward}")

                # noramlize group adv
                final_reward_mean = final_reward.mean()
                final_reward_std = final_reward.std()
                advantages = (final_reward - final_reward_mean) / (
                    final_reward_std + 1e-8
                )  # (B)

                experiences.append(
                    Experience(
                        prompt_response_ids,
                        action_log_probs.detach(),
                        advantages,
                        attention_mask,
                        action_mask,
                        num_actions,
                    )
                )

        return experiences

    @staticmethod
    def compute_approx_kl(
        log_probs: torch.Tensor,
        ref_log_probs: torch.Tensor,
        action_mask: Optional[torch.Tensor] = None,
    ):
        # GRPO uses K3 KL divergence
        log_ratio = log_probs.float() - ref_log_probs.float()
        if action_mask is not None:
            log_ratio = log_ratio * action_mask
        kl = log_ratio.exp() - 1 - log_ratio
        return kl

    def compute_core_loss(self, new_probs, old_probs, advantages, kl, action_mask=None):
        advantages = advantages.unsqueeze(1)  # shape: (B) -> (B, 1)
        ratio = (new_probs - old_probs).exp()
        loss = (
            -torch.min(
                ratio * advantages,
                ratio.clamp(1 - self.args.clip_eps, 1 + self.args.clip_eps)
                * advantages,
            )
            + self.args.beta * kl
        )
        if action_mask is None:
            return loss.mean(-1).mean()
        return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()

    def compute_loss(self, experiences):
        prompt_response_ids = torch.cat(
            [exp.prompt_response_ids for exp in experiences], dim=0
        )
        old_action_log_probs = torch.cat(
            [exp.action_log_probs for exp in experiences], dim=0
        )
        advantages = torch.cat([exp.advantages for exp in experiences], dim=0)
        attention_mask = torch.cat([exp.attention_mask for exp in experiences], dim=0)
        action_mask = torch.cat([exp.action_mask for exp in experiences], dim=0)
        num_actions = experiences[0].num_actions

        logits = self.model(prompt_response_ids, attention_mask=attention_mask).logits
        log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)
        log_probs_labels = log_probs.gather(
            dim=-1, index=prompt_response_ids[:, 1:].unsqueeze(-1)
        )
        action_log_probs = log_probs_labels.squeeze(-1)[:, -num_actions:]

        # get ref's output logits
        with torch.no_grad():
            ref_output = self.ref_model(
                prompt_response_ids, attention_mask=attention_mask
            ).logits
            ref_log_probs = F.log_softmax(ref_output[:, :-1, :], dim=-1)
            del ref_output
            torch.cuda.empty_cache()
            ref_log_probs_labels = ref_log_probs.gather(
                dim=-1, index=prompt_response_ids[:, 1:].unsqueeze(-1)
            )  # seqs[:, 1:] shape: [batch_size, seq_len-1] â€“ these are the target token IDs (next token at each position) # .unsqueeze(-1) makes that [batch_size, seq_len-1, 1] so it can be used as an index.
            ref_action_log_probs = ref_log_probs_labels.squeeze(-1)[
                :, -num_actions:
            ]  # truncate for only output part

        kl = self.compute_approx_kl(
            action_log_probs, ref_action_log_probs, action_mask=action_mask
        )
        loss = self.compute_core_loss(
            action_log_probs,
            old_action_log_probs,
            advantages,
            kl,
            action_mask=action_mask,
        )
        return loss

    def train_step(self, experiences, update_step):
        self.model.train()
        loss = self.compute_loss(experiences)
        loss = loss / self.args.gradient_accumulation_steps
        loss.backward()

        if ((update_step + 1) % self.args.gradient_accumulation_steps == 0) or (self.step + 1 == self.global_steps):
            self.optimizer.step()
            self.optimizer.zero_grad()
            writer.add_scalar("policy_loss", loss.item(), self.step)
            print(f"step {self.step} - policy_loss: {loss.item():.8f}")
        torch.cuda.empty_cache()


    def train(self):
        self.global_steps = (
            self.args.epoch
            * self.args.num_iterations * len(self.train_dataset)
            // (self.args.batch_size * self.args.gradient_accumulation_steps)
        )
        self.step = 0
        with tqdm(total=self.global_steps, desc="Training Steps") as pbar:
            for _ in range(self.args.epoch):
                dataloader = DataLoader(
                    self.train_dataset, batch_size=self.args.batch_size, shuffle=True
                )  # -> Dict[List], e.g. {'prompt': [...], 'answer': [...]}
                for idx, batch_input in enumerate(dataloader):
                    samples = self.generate_samples(batch_input)
                    # print(samples)
                    experiences = self.generate_experiences(samples)
                    self.exp_buffer[idx % self.args.gradient_accumulation_steps] = experiences # shape: (batch_size, sample_size, gc_steps), i.e. # of records used per training udpates
                    del samples

                    if (idx + 1) % self.args.gradient_accumulation_steps == 0:
                        for _ in range(self.args.num_iterations):
                            for update_step, inputs in enumerate(self.exp_buffer):
                                self.train_step(inputs, update_step)
                            self.step += 1
                            pbar.update(1)
                            if (self.step % self.args.save_steps == 0) or (self.step == self.global_steps):
                                self.model.save_pretrained(self.args.output_dir + f'/checkpoint_{self.step}')
                                self.tokenizer.save_pretrained(self.args.output_dir + f'/checkpoint_{self.step}')
                    if self.max_steps is not None and self.step >= self.max_steps:
                        print(
                            f"Reached max steps of {self.max_steps}, stopping training."
                        )
                        return

# %%
class GRPOArguments:
    output_dir = "./grpo/result"
    epoch = 1
    lr = 0.000001
    batch_size = 2  # 16
    gradient_accumulation_steps = 2  # $$\text{Effective Batch Size} = \text{Batch Size per GPU} \times \text{Gradient Accumulation Steps} \times \text{Number of GPUs}$$
    save_steps = 2000
    num_samples = 4  # num_sample genereated per prompt
    max_prompt_length = 256
    max_generate_length = 256
    clip_eps = 0.2
    beta = 0.1
    num_iterations = 3 # # of iterations for new policy model updates given the same generated samples 
    # max_steps = 15000


# %%
SYSTEM_PROMPT = """
    Use below format to answer questions
    <think>
    your thinking process
    </think>
    <answer>
    your answer
    </answer>
    """

# %%
gc.collect()
torch.cuda.empty_cache()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
writer = SummaryWriter("./grpo/result/runs")
# found that 0.5B really can't learn.. anything not even like format, so changed to 1.5B
tokenizer = AutoTokenizer.from_pretrained(
    "./Qwen2.5-1.5B-Instruct", local_files_only=True
)
policy_model = AutoModelForCausalLM.from_pretrained(
    "./Qwen2.5-1.5B-Instruct", local_files_only=True, torch_dtype=torch.bfloat16
)

# %%
dataset = GSM8KDataset("./dataset/gsm8k_chinese", tokenizer)
policy_model.to(device)
policy_model.gradient_checkpointing_enable()  # reduce memory purpose
policy_model.enable_input_require_grads()
print(f"device: {device}")

args = GRPOArguments()
trainer = GRPOTrainer(
    model=policy_model,
    args=args,
    train_dataset=dataset,
    tokenizer=tokenizer,
    reward_funcs=[correctness_reward, digit_reward, hard_format_reward, mark_reward],
    reward_weights = [0.55, 0.15, 0.15, 0.15]
)
trainer.train()


# %%
