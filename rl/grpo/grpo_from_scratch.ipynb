{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279bc02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yingyao/miniconda3/envs/transformer-practice/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author:\n",
    "Date: 25/12/24\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from typing import List, Callable, Dict, Union, Optional\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from trl.trainer import grpo_trainer\n",
    "from grpo_reward_func import correctness_reward, digit_reward, hard_format_reward, mark_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e610cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        data = load_dataset(data_path)\n",
    "        self.data = data[\"train\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        # prompt = self.tokenizer.apply_chat_template(sample['prompt'], tokenize=False, add_generation_prompt=True)\n",
    "        answer = sample[\"answer_only\"]\n",
    "        prompt = sample[\"question\"]\n",
    "        return {\"prompt\": prompt, \"answer\": answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1ddd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'Qwen2.5-0.5B-Instruct' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments()\n",
    "writer = SummaryWriter(\"./grpo/result/runs\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen2.5-0.5B-Instruct\") # Users/yingyao/Desktop/Code/GetHandsDirty.nosync\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\"Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19168f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GSM8KDataset('./dataset/gsm8k_chinese', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b850c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 72}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89cd9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Samples:\n",
    "    prompt_response_ids: torch.Tensor\n",
    "    response_ids: torch.Tensor\n",
    "    prompt: str\n",
    "    answer: str\n",
    "    attention_mask: Optional[torch.LongTensor]\n",
    "    action_mask: Optional[torch.BoolTensor]\n",
    "    num_actions: Union[torch.Tensor, int]\n",
    "    response_length: torch.Tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Experience:\n",
    "    prompt_response_ids: torch.Tensor\n",
    "    action_log_probs: torch.Tensor\n",
    "    advantages: Optional[torch.Tensor]\n",
    "    attention_mask: Optional[torch.LongTensor]\n",
    "    action_mask: Optional[torch.BoolTensor]\n",
    "    num_actions: Union[int, torch.Tensor]\n",
    "    # kl: Optional[torch.Tensor] = None\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class BufferItem:\n",
    "\n",
    "#     prompt_response_ids: torch.Tensor\n",
    "#     action_log_probs: torch.Tensor\n",
    "#     # values: torch.Tensor\n",
    "#     # returns: torch.Tensor\n",
    "#     advantages: torch.Tensor\n",
    "#     attention_mask: torch.Tensor\n",
    "#     action_mask: torch.Tensor\n",
    "#     num_actions: Union[int, torch.Tensor]\n",
    "\n",
    "# def collate_fn(batch):\n",
    "\n",
    "#     prompt_response_ids = []\n",
    "#     action_log_probs = []\n",
    "#     # values = []\n",
    "#     # returns = []\n",
    "#     advantages = []\n",
    "#     attention_mask = []\n",
    "#     action_mask = []\n",
    "\n",
    "#     for x in batch:\n",
    "#         prompt_response_ids.append(x[\"prompt_response_ids\"])\n",
    "#         action_log_probs.append(x[\"action_log_probs\"])\n",
    "#         # values.append(x[\"values\"])\n",
    "#         # returns.append(x[\"returns\"])\n",
    "#         advantages.append(x[\"advantages\"])\n",
    "#         attention_mask.append(x[\"attention_mask\"])\n",
    "#         action_mask.append(x[\"action_mask\"])\n",
    "\n",
    "#     prompt_response_ids = torch.cat(prompt_response_ids, dim=0)\n",
    "#     action_log_probs = torch.cat(action_log_probs, dim=0)\n",
    "#     # values = torch.cat(values, dim=0)\n",
    "#     # returns = torch.cat(returns, dim=0)\n",
    "#     advantages = torch.cat(advantages, dim=0)\n",
    "#     attention_mask = torch.cat(attention_mask, dim=0)\n",
    "#     action_mask = torch.cat(action_mask, dim=0)\n",
    "\n",
    "#     return BufferItem(\n",
    "#         prompt_response_ids,\n",
    "#         action_log_probs,\n",
    "#         # values,\n",
    "#         # returns,\n",
    "#         advantages,\n",
    "#         attention_mask,\n",
    "#         action_mask,\n",
    "#         action_mask.size(1),\n",
    "#     )\n",
    "\n",
    "# class PromptDataset(Dataset):\n",
    "#     def __init__(self, prompts, tokenizer, apply_chat_template=True):\n",
    "#         self.prompts = prompts\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.final_prompts = []\n",
    "#         for prompt in self.prompts:\n",
    "#             if apply_chat_template:\n",
    "#                 content = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#                 prompt = self.tokenizer.apply_chat_template(\n",
    "#                     content, tokenize=False, apply_chat_template=True\n",
    "#                 )\n",
    "#             else:\n",
    "#                 prompt = self.tokenizer.bos_token + prompt\n",
    "#             self.final_prompts.append(prompt)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.prompts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.final_prompts[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc7d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "    Use below format to answer questions\n",
    "    <think>\n",
    "    your thinking process\n",
    "    </think>\n",
    "    <answer>\n",
    "    your answer\n",
    "    </answer>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        reward_funcs: List[Callable] = None, # here doesn't include loaded reward model from_pretrained\n",
    "        ref_model=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.model = model.to(model.device)\n",
    "        self.args = args\n",
    "        self.ref_model = (\n",
    "            ref_model.to(model.device).eval()\n",
    "            if ref_model is not None\n",
    "            else model.to(model.device).eval()\n",
    "        )\n",
    "        self.train_dataset = train_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        self.reward_funcs = reward_funcs\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.lr)\n",
    "        self.max_steps = self.args.max_steps if hasattr(self.args, 'max_steps') else None\n",
    "\n",
    "    def generate_samples(\n",
    "        self,\n",
    "        inputs: List[Dict[str, str]],\n",
    "    ) -> List[Samples]:\n",
    "        samples_list = []\n",
    "        self.model.eval()\n",
    "        for prompt, answer in zip(inputs[\"prompt\"], inputs[\"answer\"]):\n",
    "            total_length = self.args.max_generate_length + self.args.max_prompt_length\n",
    "            input_text = self.tokenizer.apply_chat_template(\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=False,\n",
    "            )\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.args.max_prompt_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            with torch.no_grad():\n",
    "                prompt_response_ids = self.model.generate(\n",
    "                    **inputs.to(self.model.device),\n",
    "                    max_new_tokens=self.args.max_generate_length,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    num_return_sequences=self.args.num_sample_generations, # generate a group responses\n",
    "                    do_sample=True,\n",
    "                )\n",
    "            if (\n",
    "                prompt_response_ids.size(1)\n",
    "                >= total_length\n",
    "            ):\n",
    "                prompt_response_ids = prompt_response_ids[\n",
    "                    :, : total_length\n",
    "                ]\n",
    "            else:\n",
    "                prompt_response_ids = torch.cat(\n",
    "                    [\n",
    "                        prompt_response_ids,\n",
    "                        torch.full(\n",
    "                            (\n",
    "                                prompt_response_ids.size(0),\n",
    "                                total_length\n",
    "                                - prompt_response_ids.size(1),\n",
    "                            ),\n",
    "                            fill_value=self.tokenizer.pad_token_id,\n",
    "                            device=prompt_response_ids.device,\n",
    "                        ),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "            attention_mask = (prompt_response_ids.ne(self.tokenizer.pad_token_id)).to(\n",
    "                dtype=torch.long\n",
    "            )  # masked since beginning of the prompt\n",
    "            response_ids = prompt_response_ids[:, input_ids.size(1) :]\n",
    "            action_mask = (\n",
    "                response_ids.ne(self.tokenizer.pad_token_id)\n",
    "                & response_ids.ne(self.tokenizer.eos_token_id)\n",
    "            ).to(\n",
    "                dtype=torch.long\n",
    "            )  # masked since beginning of the answer\n",
    "\n",
    "            # each samples is a \"group\" per prompt in GRPO\n",
    "            samples = Samples(\n",
    "                prompt_response_ids=prompt_response_ids, # shape: (num_sample_generations, total_length)\n",
    "                response_ids=response_ids,\n",
    "                prompt=prompt, # shape: (1)\n",
    "                answer=answer, # shape: (1)\n",
    "                attention_mask=attention_mask, # shape: (num_sample_generations, total_length)\n",
    "                action_mask=action_mask, # shape: (num_sample_generations, max_generate_length)\n",
    "                num_actions=action_mask.size(1), # shape: (1)\n",
    "                response_length=action_mask.float().sum(dim=-1), # shape: (num_sample_generations, 1)\n",
    "            )\n",
    "            samples_list.append(samples)\n",
    "        return samples_list\n",
    "\n",
    "    def generate_experiences(self, samples_list: List[Samples]):\n",
    "        self.model.eval()\n",
    "        experiences = []\n",
    "        for samples in samples_list:\n",
    "            prompt_response_ids = samples.prompt_response_ids # shape: (num_sample_generations, total_length)\n",
    "            prompt = samples.prompt # shape: (1)\n",
    "            answer = samples.answer # shape: (1)\n",
    "            response_ids = samples.response_ids \n",
    "            attention_mask = samples.attention_mask\n",
    "            action_mask = samples.action_mask\n",
    "            num_actions = samples.num_actions # shape: (1)\n",
    "            with torch.no_grad():\n",
    "                # get output logits\n",
    "                outputs = self.model(prompt_response_ids, attention_mask=attention_mask)\n",
    "                log_probs = F.softmax(outputs.logits[:, :-1, :], dim=-1)  # (B, S, V)\n",
    "                log_probs_labels = log_probs.gather(\n",
    "                    dim=-1, index=prompt_response_ids[:, 1:].unsqueeze(-1)\n",
    "                )  # (B, S, 1)\n",
    "                action_log_probs = log_probs_labels.squeeze(-1)[:, -num_actions:] # truncate for only output part\n",
    "\n",
    "                responses = self.tokenizer.batch_decode(response_ids, skip_special_tokens=True)\n",
    "                \n",
    "                # calculate reward\n",
    "                # here ignores weights across different reward functions\n",
    "                prompts = [prompt] * response_ids.size(0)\n",
    "                answers = [answer] * response_ids.size(0)\n",
    "                final_reward = torch.zeros(response_ids.size(0), device=self.model.device)\n",
    "                for reward_func in self.reward_funcs:\n",
    "                    reward = reward_func(prompts, responses, answers)\n",
    "                    # Convert list to tensor and accumulate\n",
    "                    final_reward += torch.tensor(reward, device=self.model.device, dtype=torch.float)\n",
    "                \n",
    "                final_reward /= len(self.reward_funcs)\n",
    "                print(f'prompt: {prompt}, grouped rewards: {final_reward}')\n",
    "\n",
    "                # noramlize group adv\n",
    "                final_reward_mean = final_reward.mean()\n",
    "                final_reward_std = final_reward.std()\n",
    "                advantages = (final_reward - final_reward_mean) / (final_reward_std + 1e-8)  # (B)\n",
    "\n",
    "                experiences.append(\n",
    "                    Experience(\n",
    "                        prompt_response_ids,\n",
    "                        action_log_probs.detach(),\n",
    "                        advantages,\n",
    "                        attention_mask,\n",
    "                        action_mask,\n",
    "                        num_actions\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return experiences\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_approx_kl(\n",
    "        log_probs: torch.Tensor,\n",
    "        ref_log_probs: torch.Tensor,\n",
    "        action_mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        # GRPO uses K3 KL divergence\n",
    "        log_ratio = log_probs.float() - ref_log_probs.float()\n",
    "        if action_mask is not None:\n",
    "            log_ratio = log_ratio * action_mask\n",
    "        kl = log_ratio.exp() - 1 - log_ratio\n",
    "        return kl\n",
    "    \n",
    "    def compute_core_loss(self, new_probs, old_probs, advantages, kl, action_mask=None):\n",
    "        advantages = advantages.unsqueeze(1)  # shape: (B) -> (B, 1)\n",
    "        ratio = (new_probs - old_probs).exp()\n",
    "        loss = (\n",
    "            -torch.min(\n",
    "                ratio * advantages, ratio.clamp(1 - self.args.clip_eps, 1 + self.args.clip_eps) * advantages\n",
    "            )\n",
    "            + self.args.beta * kl\n",
    "        )\n",
    "        if action_mask is None:\n",
    "            return loss.mean(-1).mean()\n",
    "        return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()\n",
    "\n",
    "    \n",
    "    def compute_loss(self, experiences):\n",
    "        prompt_response_ids = torch.cat([exp.prompt_response_ids for exp in experiences], dim=0)\n",
    "        old_action_log_probs = torch.cat([exp.action_log_probs for exp in experiences], dim=0)\n",
    "        advantages = torch.cat([exp.advantages for exp in experiences], dim=0)\n",
    "        attention_mask = torch.cat([exp.attention_mask for exp in experiences], dim=0)\n",
    "        action_mask = torch.cat([exp.action_mask for exp in experiences], dim=0)\n",
    "        num_actions = experiences[0].num_actions\n",
    "        \n",
    "        logits = self.model(prompt_response_ids, attention_mask=attention_mask).logits\n",
    "        log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)\n",
    "        log_probs_labels = log_probs.gather(dim=-1, index=prompt_response_ids[:, 1:].unsqueeze(-1))\n",
    "        action_log_probs = log_probs_labels.squeeze(-1)[:, -num_actions:]\n",
    "\n",
    "        # get ref's output logits\n",
    "        ref_output = self.ref_model(prompt_response_ids, attention_mask=attention_mask).logits\n",
    "        ref_log_probs = F.log_softmax(ref_output[:, :-1, :], dim=-1)\n",
    "        ref_log_probs_labels = ref_log_probs.gather(\n",
    "            dim=-1, index=prompt_response_ids[:, 1:].unsqueeze(-1)\n",
    "        )  # seqs[:, 1:] shape: [batch_size, seq_len-1] â€“ these are the target token IDs (next token at each position) # .unsqueeze(-1) makes that [batch_size, seq_len-1, 1] so it can be used as an index.\n",
    "        ref_action_log_probs = ref_log_probs_labels.squeeze(-1)[:, -num_actions:] # truncate for only output part\n",
    "\n",
    "        kl = self.compute_approx_kl(\n",
    "            action_log_probs, ref_action_log_probs, action_mask=action_mask\n",
    "        )\n",
    "        loss = self.compute_core_loss(\n",
    "            action_log_probs, old_action_log_probs, advantages, kl, action_mask=action_mask\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, experiences):\n",
    "        self.model.train()\n",
    "        loss = self.compute_loss(experiences)\n",
    "        loss = loss / self.args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        print(f\"step: {self.step}  policy_loss: {loss.item():.3f}\")\n",
    "\n",
    "        if (self.step + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            print(f\"step {self.step}: optimizer updated!\")\n",
    "        else:\n",
    "            print(f\"step {self.step}: gradients accumulated...\")\n",
    "        \n",
    "        if (self.step + 1) == self.global_steps:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "    def train(self):\n",
    "        self.global_steps = self.args.epoch * len(self.train_dataset) // (self.args.batch_size * self.args.gradient_accumulation_steps)\n",
    "        self.step = 0\n",
    "        for _ in range(self.args.epoch):\n",
    "            dataloader = DataLoader(\n",
    "                self.train_dataset, batch_size=self.args.batch_size, shuffle=True\n",
    "            ) # -> Dict[List], e.g. {'prompt': [...], 'answer': [...]}\n",
    "            for _, batch_input in enumerate(dataloader):\n",
    "                samples = self.generate_samples(batch_input)\n",
    "                # print(samples)\n",
    "                experiences = self.generate_experiences(samples)         \n",
    "                # self.exps_buffer[idx % self.args.gradient_accumulation_steps] = experiences\n",
    "                # for step, exp_buffer in enumerate(self.exps_buffer):\n",
    "                self.train_step(experiences)\n",
    "                self.step += 1\n",
    "                if self.max_steps is not None and self.step >= self.max_steps:\n",
    "                    print(f\"Reached max steps of {self.max_steps}, stopping training.\")\n",
    "                    return\n",
    "                if self.step % self.args.save_steps == 0:\n",
    "                    self.model.save_pretrained(self.args.output_dir + f'/checkpoint_{self.step}')\n",
    "                    self.tokenizer.save_pretrained(self.args.output_dir + f'/checkpoint_{self.step}')\n",
    "                del samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fad5f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class GRPOArguments:\n",
    "    output_dir=\"./grpo/result\"\n",
    "    epoch=1\n",
    "    lr = 0.000001\n",
    "    batch_size=2  # 16\n",
    "    gradient_accumulation_steps=2  # $$\\text{Effective Batch Size} = \\text{Batch Size per GPU} \\times \\text{Gradient Accumulation Steps} \\times \\text{Number of GPUs}$$\n",
    "    save_steps=100\n",
    "    num_sample_generations=4  # num_sample genereated per prompt\n",
    "    max_prompt_length=256\n",
    "    max_generate_length=512\n",
    "    reward_weights=None  # only applied if multiple reward funcs are used\n",
    "    clip_eps=0.2\n",
    "    beta=0.1\n",
    "    max_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Bob is building a garden on his land, and he wants it to be fenced in to keep out varmints. The garden is a rectangular plot 225 feet long by 125 feet wide. He also wants to have a small 3-foot wide gate to walk through and a larger 10-foot wide gate to move his gardening equipment through. How much fencing is Bob going to need to fence in his garden, taking into account the two gates?, grouped rewards: tensor([0., 0., 0., 0.])\n",
      "prompt: Kelly needs school supplies to teach her class for an art project. She has 8 students and they will need 3 pieces of construction paper each. In addition to the construction paper, she needs to buy 6 bottles of glue for everyone to share. After Kelly purchases these supplies, she dropped half of them down a storm drain. Class is about to start, but she manages to rush to the store and buy 5 more pieces of construction paper. How many supplies are left?, grouped rewards: tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args = GRPOArguments()\n",
    "trainer = GRPOTrainer(model=policy_model,\n",
    "                        args=args,\n",
    "                        train_dataset=dataset,\n",
    "                        tokenizer=tokenizer,\n",
    "                        reward_funcs = [correctness_reward, digit_reward, hard_format_reward, mark_reward])\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113d16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
